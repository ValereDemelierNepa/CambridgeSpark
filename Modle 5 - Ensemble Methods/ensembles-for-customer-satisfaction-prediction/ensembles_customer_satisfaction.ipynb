{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Ensembles for Customer Satisfaction Prediction"]},{"cell_type":"markdown","metadata":{},"source":["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n","\n","KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n","\n","* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n","* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n","\n","You will find instructions below about how to define each variable.\n","\n","Once you're happy with your code, upload your notebook to KATE to check your feedback."]},{"cell_type":"markdown","metadata":{},"source":["Businesses can improve their services by tailoring them to individual customers. One important factor is knowing when customers are dissatisfied. Based on their records, one can use machine learning tools to make predictions about which customers are more at risk of being dissatisfied than others. Such predictions allow for individualized actions that may help retain customers and will improve quality."]},{"cell_type":"markdown","metadata":{},"source":["In this assignment, we will build a prediction model for bank account owners' satisfaction. The record includes more than 300 features for each client, including variable related to their balance and which banking operations they have performed. Many of these variables are sparse; some numerical, some categorical. \n","\n","Ensemble methods based on decision trees, such as random forests and boosting algorithms, have been very successful in modeling such heterogeneous tabular data. To learn how these models work, you will implement them step-by-step, and see how the performance of your predictions improve."]},{"cell_type":"markdown","metadata":{},"source":["### Load the data"]},{"cell_type":"markdown","metadata":{},"source":["Load the data in `data/train_data.csv` with `pandas`. Inspect its content with `.head()`, `.shape` and other methods of your choice."]},{"cell_type":"code","execution_count":1,"metadata":{"scrolled":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>var3</th>\n","      <th>var15</th>\n","      <th>imp_ent_var16_ult1</th>\n","      <th>imp_op_var39_comer_ult1</th>\n","      <th>imp_op_var39_comer_ult3</th>\n","      <th>imp_op_var40_comer_ult1</th>\n","      <th>imp_op_var40_comer_ult3</th>\n","      <th>imp_op_var40_efect_ult1</th>\n","      <th>imp_op_var40_efect_ult3</th>\n","      <th>...</th>\n","      <th>saldo_medio_var33_hace2</th>\n","      <th>saldo_medio_var33_hace3</th>\n","      <th>saldo_medio_var33_ult1</th>\n","      <th>saldo_medio_var33_ult3</th>\n","      <th>saldo_medio_var44_hace2</th>\n","      <th>saldo_medio_var44_hace3</th>\n","      <th>saldo_medio_var44_ult1</th>\n","      <th>saldo_medio_var44_ult3</th>\n","      <th>var38</th>\n","      <th>TARGET</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>47739</td>\n","      <td>2</td>\n","      <td>29</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>46565.040000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4212</td>\n","      <td>2</td>\n","      <td>38</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>90736.770000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>48967</td>\n","      <td>2</td>\n","      <td>23</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>117310.979016</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11077</td>\n","      <td>2</td>\n","      <td>23</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>172107.720000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17475</td>\n","      <td>2</td>\n","      <td>26</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>67983.570000</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 371 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n","0       47739     2     29                 0.0                      0.0   \n","1        4212     2     38                 0.0                      0.0   \n","2       48967     2     23                 0.0                      0.0   \n","3       11077     2     23                 0.0                      0.0   \n","4       17475     2     26                 0.0                      0.0   \n","\n","   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n","0                      0.0                      0.0                      0.0   \n","1                      0.0                      0.0                      0.0   \n","2                      0.0                      0.0                      0.0   \n","3                      0.0                      0.0                      0.0   \n","4                      0.0                      0.0                      0.0   \n","\n","   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  ...  \\\n","0                      0.0                      0.0  ...   \n","1                      0.0                      0.0  ...   \n","2                      0.0                      0.0  ...   \n","3                      0.0                      0.0  ...   \n","4                      0.0                      0.0  ...   \n","\n","   saldo_medio_var33_hace2  saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n","0                      0.0                      0.0                     0.0   \n","1                      0.0                      0.0                     0.0   \n","2                      0.0                      0.0                     0.0   \n","3                      0.0                      0.0                     0.0   \n","4                      0.0                      0.0                     0.0   \n","\n","   saldo_medio_var33_ult3  saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n","0                     0.0                      0.0                      0.0   \n","1                     0.0                      0.0                      0.0   \n","2                     0.0                      0.0                      0.0   \n","3                     0.0                      0.0                      0.0   \n","4                     0.0                      0.0                      0.0   \n","\n","   saldo_medio_var44_ult1  saldo_medio_var44_ult3          var38  TARGET  \n","0                     0.0                     0.0   46565.040000       0  \n","1                     0.0                     0.0   90736.770000       0  \n","2                     0.0                     0.0  117310.979016       0  \n","3                     0.0                     0.0  172107.720000       0  \n","4                     0.0                     0.0   67983.570000       0  \n","\n","[5 rows x 371 columns]"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","df = pd.read_csv(\"data/train_data.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["(66020, 371)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df.shape"]},{"cell_type":"markdown","metadata":{},"source":["#### Target variable\n","\n","The last column, named `TARGET`, is the variable to be predicted. `TARGET=1` represents a dissatisfied customer. Inspect the target column with `.value_counts()`. \n","\n","What is the proportion of dissatisfied customers? Is the dataset balanced or imbalanced?"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["TARGET\n","0    63408\n","1     2612\n","Name: count, dtype: int64"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.value_counts(\"TARGET\")"]},{"cell_type":"markdown","metadata":{},"source":["### Note on dataset properties\n","\n","As you can see, the dataset is highly imbalanced: there are only 2.6k positive entries and 63.4k negative entries. It definitely should be addressed in the models by introducing class_weight parameter where possible (there are different ways it can be done - feel free check it out in sklearn documentation).\n","\n","If that is not possible to introduce class weights for the model due to the model type, be ready to the permanent majority class vote in the output. This can be addressed by tweaking the model parameters."]},{"cell_type":"markdown","metadata":{},"source":["Separate the data into features `X` and target `y`. Split the data into training and validation sets, with validation set of 5000 samples, with stratified split to keep the same level of imbalance.\n","\n","*Hint: you may use `train_test_split()` for stratified splits.*"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X = df.drop([\"TARGET\", \"Unnamed: 0\"], axis=1)\n","y = df[\"TARGET\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=5000, stratify=y)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Basic modelling pipeline\n","\n","Implement a basic modelling pipeline for a Decision Tree Classifier, fitting the training data and printing the training and validation accuracy."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["0.928"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Decision tree\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","model = DecisionTreeClassifier()\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","accuracy_score(y_test, y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["Note that the prediction score is quite high, even for this very simple model. Take a moment to think why this high score is not that significant.\n","\n","#### ROC curve metric\n","\n","Change your scoring metric to `roc_auc_score`, which calculates the area below the ROC curve of your **prediction probabilities**, instead of using the binary prediction decisions.\n","\n","*Hint: Use the probabilities for `y = True` (not `y = False`).*"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["0.5787860908123299"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import roc_auc_score\n","y_pred = model.predict_proba(X_test)\n","# Use probability of positive class\n","roc_auc_score(y_test, y_pred[:, 1])"]},{"cell_type":"markdown","metadata":{},"source":["#### Baseline score for random predictions\n","\n","Calculate the ROC AUC for random uniform prediction probabilities. \n","\n","Is the Decision Tree better? Based on the training and validation scores, what is the problem with the Decision Tree model?\n","\n","*Hint: You can use `np.random.uniform`.*"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["0.4875227464402798"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Calculate the ROC AUC for random uniform prediction probabilities.\n","# Is the Decision Tree better? Based on the training and validation scores, what is the problem with the Decision Tree model?\n","roc_auc_score(y_train, np.random.uniform(size=len(y_train)))"]},{"cell_type":"markdown","metadata":{},"source":["Create a function named `test_model(model, X_train, y_train, X_test, y_test)` that performs the basic prediction pipeline, receiving as argument the model and data, fitting the training data, and returning the training and test prediction scores. Check that it works with the Decision Tree model."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ROC AUC: 0.5893304136744371\n"]}],"source":["def test_model(model, X_train, y_train, X_test, y_test):\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict_proba(X_test)\n","    print(\"ROC AUC:\", roc_auc_score(y_test, y_pred[:, 1]))\n","\n","test_model(DecisionTreeClassifier(), X_train, y_train, X_test, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["## Optimizing decision trees "]},{"cell_type":"markdown","metadata":{},"source":["We can improve the prediction model by setting up the Decision Tree. Check the arguments available for the `DecisionTreeClassifier` class. \n","\n","Which arguments do you think could improve the validation score? Optimize your model by changing the meta-parameters. Inspect the most important meta-parameter by calculating the training and validation score for different values."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ROC AUC: 0.8296048784386978\n"]}],"source":["test_model(DecisionTreeClassifier(max_depth=5, min_samples_split=250, min_samples_leaf = 100),\n","            X_train, y_train, X_test, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["To evaluate your models, we will test your data on a testing set. Load the test at `data/test_data.csv`."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["test_data = pd.read_csv('data/test_data.csv')"]},{"cell_type":"markdown","metadata":{},"source":[" Calculate the prediction probabilities for the test data for the best Decision Tree, saving them in a variable named `dtc_preds`. `dtc_preds` should be an numpy array a single dimension."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["model = DecisionTreeClassifier(max_depth=5, min_samples_split=250, min_samples_leaf = 100)\n","model.fit(X, y)\n","dtc_preds = model.predict(test_data)"]},{"cell_type":"markdown","metadata":{},"source":["### Bagging and Random Forests"]},{"cell_type":"markdown","metadata":{},"source":["While Decision Trees are prone to overfitting, their ensemble can be powerful predictors. Random Forests are essentially Bagging ensembles of decision trees, using the average prediction of the multiple decision trees base models, each trained with a different set of data samples.\n","\n","You will create a Bagging model class, named `myBagging`, filling the class structure below.\n","\n","The `.fit()` method should fit each base model with a bootstrap sample of the data (with replacement), with data size proportional by the meta-parameter `subsample`. That is, if `subsample=0.5`, each base model should get half the total number of samples.\n","\n","The `.predict_proba()` method should estimate and average the prediction probabilities of the base models.\n","\n","*Hint: You can use the `resample()` function for creating bootstrap samples.*"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class myBagging:\n","    def __init__(self, base_models, subsample = 1.):\n","        self.n_models = len(base_models)\n","        self.base_models = base_models\n","        self.subsample = subsample\n","\n","    def fit(self, X, y):\n","        '''Loop over base models, generate a bootstrap sample of the data with 'resample()',\n","           and fit them to the data.\n","\n","           To access the variables inside the myBagging class, use the 'self.' prefix,\n","           i.e. self.base_models, self.n_models and self.subsample\n","        '''\n","        for i in range(self.n_models):\n","            sample = X.sample(frac=self.subsample, replace=True), y.sample(frac=self.subsample, replace=True)\n","            self.base_models[i].fit(sample[0], sample[1])\n","        pass\n","\n","    def predict_proba(self, X):\n","        '''Return the ensemble predictions, given by the average prediction probability over base models.\n","           It should be an array with the length of the dataset.'''\n","        preds = np.zeros((len(X), 2))\n","        for i in range(self.n_models):\n","            preds += self.base_models[i].predict_proba(X)\n","\n","        return preds / self.n_models\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Run and score a Random Forest, with 10 base Decision Trees, with maximum depth 10 and subsample 0.5. Use your `myBagging` class and `test_model()`."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ROC AUC: 0.4285614369433612\n"]}],"source":["#Run the previous designed class\n","base_models = [DecisionTreeClassifier(max_depth=5, min_samples_split=250, min_samples_leaf = 100) for i in range(10)]\n","myBagging_int = myBagging(base_models, subsample = 0.5)\n","test_model(myBagging_int, X_train, y_train, X_test, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Extra-Trees\n","\n","Extremely Randomized Trees are decision trees in which, at each node split during training , only a fraction of the features is considered for the optimal split (e.g. for optimal Gini gain). This functionality is implemented on `sklearn` under the parameter `max_features`. \n","\n","Run and score a Extra-Trees version of your Random Forest, by changing the `max_features` parameter."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ROC AUC: 0.5130043668673406\n"]}],"source":["#Run the previous designed class\n","base_models = [DecisionTreeClassifier(max_depth=5, min_samples_split=200, min_samples_leaf = 100, max_features=250) for i in range(10)]\n","myBagging_ext = myBagging(base_models, subsample = 0.5)\n","test_model(myBagging_ext, X_train, y_train, X_test, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### Sklearn comparison"]},{"cell_type":"markdown","metadata":{},"source":["For comparison, run and score the `sklearn` implementation, `RandomForestClassifier`."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["0.8394361145818872"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.ensemble import RandomForestClassifier\n","model = RandomForestClassifier(n_estimators=10, max_depth=5, min_samples_split=250, min_samples_leaf = 100, max_features=250)\n","model.fit(X_train, y_train)\n","y_pred = model.predict_proba(X_test)\n","roc_auc_score(y_test, y_pred[:, 1])"]},{"cell_type":"markdown","metadata":{},"source":["### Optimize your Random Forest"]},{"cell_type":"markdown","metadata":{},"source":["Optimize your Random Forest meta-parameters, both of the myBagging and Decision Trees, and make your predictions for the test data, saving the predictions under `rf_preds`."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["model = RandomForestClassifier(n_estimators = 53, max_depth = 23, min_samples_split = 47, min_samples_leaf = 31, max_features = 141)\n","test_model(model, X_train, y_train, X_test, y_test)\n","rf_preds = model.predict(test_data)"]},{"cell_type":"markdown","metadata":{},"source":["Note that including more decision trees improve performance but increases the computational cost of training linearly. The `max_depth` and `max_features` arguments can heavily cut the training time, by reducing the tree size and number of features considered at each split."]},{"cell_type":"markdown","metadata":{},"source":["## Gradient Boosting"]},{"cell_type":"markdown","metadata":{},"source":["We will now implement a more sophisticated ensemble, Gradient Boosting, in which the base models are trained sequentially. Each new base model predicts what previous base models missed. \n","\n","As gradient boosting requires a continuous gradient, it can only use regression models for the base learner. \n","\n","For this exercise, we will perform regression directly on the 0-1 class labels, and treat the raw outputs as probabilities. \n","\n","We will try to setup the base models to optimise the MSE loss function against the class-labels, for which the gradient becomes simply the residual errors. \n","\n","When applied to probabilities, the MSE is known as the Brier score. \n","\n","Whilst performing this exercise, have a think about whether this is a robust approach. \n","\n","If not, what would you change either to your base-learners, meta-algorithm, or evaluation metrics to make this more robust?\n","\n","You will have a chance to implement your suggestions tomorrow!\n","\n","In the below structure, fill the `.fit()` and `.predict_proba()` functions. "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["class myGradientBoosting:\n","\n","    def __init__(self, base_models, learning_rate=0.5):\n","        self.n_models = len(base_models)\n","        self.models = base_models\n","        self.learning_rate = learning_rate\n","\n","    def fit(self, x, y):\n","        ''' The `.fit()` function should loop over each base model\n","         fitting it to the residual of the ensemble predictions so far, for the MSE loss:\n","\n","         predictions = 0\n","         for each base model:\n","             residual = y - predictions\n","             fit base model and make new predictions\n","             predictions = predictions + learning_rate * new_prediction\n","        '''\n","        predictions = np.zeros(y.shape) # Ensure predictions are correctly shaped for operations\n","        for i in range(self.n_models):\n","            residual = y - predictions\n","            self.models[i].fit(x, residual)\n","            predictions += self.learning_rate * self.models[i].predict(x)\n","\n","        pass\n","\n","    def predict_proba(self, x):\n","        ''' Generate the ensemble prediction, by looping over each base model.\n","            Get their predictions and sum them, scaled by the learning rate.\n","\n","            Trick: Regressor models return only one prediction (instead of two probabilities in the Classifiers).\n","                   To make your class compatible with test_model(), you can repeat the predictions, e.g.:\n","                   predictions.reshape(-1,1).repeat(2,axis=1)'''\n","        predictions = 0\n","        for model in self.models:\n","            model_pred = model.predict(x)\n","            predictions += self.learning_rate * model_pred\n","\n","        predictions = predictions.reshape(-1, 1).repeat(2, axis=1)\n","        return predictions"]},{"cell_type":"markdown","metadata":{},"source":["Run and score a Gradient Boosting model, with 20 base decision trees, with maximum depth 5, maximum feature 0.5 and learning rate 0.5. Use your `myGradientBoosting` class and `test_model()`. "]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["0.8265379744971582"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.tree import DecisionTreeRegressor\n","base_models = [DecisionTreeRegressor(max_depth=5, max_features=250) for i in range(20)]\n","myGB = myGradientBoosting(base_models, learning_rate=0.5)\n","myGB.fit(X_train, y_train)\n","pred = myGB.predict_proba(X_test)\n","roc_auc_score(y_test, pred[:, 1])"]},{"cell_type":"markdown","metadata":{},"source":["For comparison, run and score the `sklearn` implementation, `GradientBoostingClassifier`."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ROC AUC: 0.8317383539686747\n"]}],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","model = GradientBoostingClassifier(n_estimators=20, max_depth=5, max_features=250, learning_rate=0.5)\n","test_model(model, X_train, y_train, X_test, y_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["Optimize your myGradientBoosting and decision tree meta-parameters, and make your predictions for the test data, saving the predictions under `gb_preds`."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["gb_preds = model.predict(test_data)"]},{"cell_type":"markdown","metadata":{},"source":["Try to think about the difference between your implementation and the GradientBoostingClassifier.\n","\n","Are there any fundamental differences? If so, why?\n","\n","You could try looking at the distribution of your output probabilities for each model."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":4}
